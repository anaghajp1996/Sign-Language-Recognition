# Sign-Language-Recognition
## A Machine Learning project aiming to recognise sign language using CNN

The aim of this project is to use Convolutional Neural Networks to recognize sign language. The model will classify the multiple hand gestures used for fingerspelling in sign language. In the Sign Language dataset, the machine learning algorithm is trained using a portion of the image data and testing is done on the rest of the data portion. 
Communication is a very important aspect, allowing each other to express themselves. Different forms of communication include writing, speaking, visual aids, and gestures. But unfortunately, for the people who are hard of hearing can only communicate through hand gestures, visual aids, etc. This also leads to a gap between the hearing majority and the deaf population. The alternative of written communication between these two groups is quite cumbersome and time consuming. This can potentially cause problems during emergency situations, where it is highly necessary to communicate quickly.
The goal of this project is to make a contribution to the sign language recognition. For this, we make use of the fingerspelling data, which essentially spells out words letter by letter. It enables communication of words such as addresses or names which have no meaning in word level association. The image dataset consists of labels and pixel values in rows. It has 24 classifications which excludes two letters- J and Z as they require motion. The images are represented as 28X28 pixels with greyscale values ranging from 0 to 255. 
